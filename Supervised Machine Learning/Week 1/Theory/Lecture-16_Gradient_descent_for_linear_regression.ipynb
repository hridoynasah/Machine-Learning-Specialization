{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Introduction: What Are We Doing?**\n",
    "\n",
    "We‚Äôre combining three things:\n",
    "\n",
    "- **Linear Regression Model** üìà: Predicts a straight line to fit the training data.\n",
    "- **Squared Error Cost Function** üßÆ: Measures how far off the predictions are from the actual values.\n",
    "- **Gradient Descent** ‚öôÔ∏è: An algorithm to minimize the cost function by adjusting the parameters $ w $ (weight) and $ b $ (bias).\n",
    "\n",
    "### **2. The Goal**\n",
    "\n",
    "We want to train the linear regression model so it fits the data by using:\n",
    "\n",
    "- The **cost function** to measure the error.\n",
    "- **Gradient descent** to reduce that error and find the best $ w $ and $ b $.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Linear Regression Model and Cost Function**\n",
    "\n",
    "#### **Linear Regression Model** üìä\n",
    "\n",
    "The formula for a linear regression model is:\n",
    "\n",
    "$$\n",
    "f(w, b, x) = w \\cdot x + b\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $ w $: Weight (how much $ x $ affects the prediction).\n",
    "- $ b $: Bias (shifts the line up or down).\n",
    "- $ x $: Input feature (data point).\n",
    "\n",
    "#### **Squared Error Cost Function** üßæ\n",
    "\n",
    "The cost function measures how far predictions are from actual values. The formula is:\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( f(w, b, x^i) - y^i \\right)^2\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $ m $: Number of training examples.\n",
    "- $ y^i $: Actual value.\n",
    "- $ f(w, b, x^i) $: Predicted value.\n",
    "\n",
    "The $ \\frac{1}{2} $ is included to simplify calculations when taking derivatives.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Gradient Descent**\n",
    "\n",
    "Gradient descent helps find the best $ w $ and $ b $ by minimizing the cost function $ J(w, b) $.\n",
    "\n",
    "#### **Update Rules**\n",
    "\n",
    "The parameters $ w $ and $ b $ are updated using these formulas:\n",
    "\n",
    "$$\n",
    "w = w - \\alpha \\cdot \\frac{\\partial J}{\\partial w}, \\quad b = b - \\alpha \\cdot \\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "\n",
    "- $ \\alpha $: Learning rate (step size).\n",
    "- $ \\frac{\\partial J}{\\partial w} $: Derivative of $ J $ with respect to $ w $.\n",
    "- $ \\frac{\\partial J}{\\partial b} $: Derivative of $ J $ with respect to $ b $.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Calculating Derivatives**\n",
    "\n",
    "Using calculus, the derivatives are derived as follows:\n",
    "\n",
    "#### **Derivative with Respect to $ w $:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( f(w, b, x^i) - y^i \\right) \\cdot x^i\n",
    "$$\n",
    "\n",
    "- $ f(w, b, x^i) - y^i $: Error term (difference between predicted and actual values).\n",
    "- $ x^i $: Input feature.\n",
    "\n",
    "#### **Derivative with Respect to $ b $:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( f(w, b, x^i) - y^i \\right)\n",
    "$$\n",
    "\n",
    "- This is similar to the formula for $ w $, but without the $ x^i $ term.\n",
    "\n",
    "These formulas are plugged into the gradient descent algorithm to compute updates for $ w $ and $ b $.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Convex Cost Function and Global Minimum**\n",
    "\n",
    "The squared error cost function is **convex** ü•£ (bowl-shaped). This means:\n",
    "\n",
    "- It has only **one global minimum** (the lowest point).\n",
    "- Gradient descent will always converge to this point if the learning rate is set appropriately.\n",
    "\n",
    "Unlike some functions with multiple local minima, there‚Äôs no risk of getting stuck in the wrong place.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Algorithm Recap**\n",
    "\n",
    "Here‚Äôs the **gradient descent algorithm** step-by-step:\n",
    "\n",
    "1. Initialize $ w $ and $ b $ (randomly or as zeros).\n",
    "2. Compute the cost $ J(w, b) $.\n",
    "3. Calculate derivatives $ \\frac{\\partial J}{\\partial w} $ and $ \\frac{\\partial J}{\\partial b} $.\n",
    "4. Update $ w $ and $ b $ using the formulas:\n",
    "   $$\n",
    "   w = w - \\alpha \\cdot \\frac{\\partial J}{\\partial w}, \\quad b = b - \\alpha \\cdot \\frac{\\partial J}{\\partial b}\n",
    "   $$\n",
    "5. Repeat steps 2‚Äì4 until the cost $ J(w, b) $ stops decreasing (convergence).\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "- The **linear regression model** predicts $ y $ using $ f(w, b, x) = w \\cdot x + b $.\n",
    "- The **cost function** $ J(w, b) $ measures prediction errors.\n",
    "- **Gradient descent** minimizes $ J(w, b) $ by adjusting $ w $ and $ b $.\n",
    "- The derivatives $ \\frac{\\partial J}{\\partial w} $ and $ \\frac{\\partial J}{\\partial b} $ are calculated using calculus.\n",
    "- The cost function is convex, so gradient descent always finds the global minimum.\n",
    "\n",
    "---\n",
    "\n",
    "### **Interactive Note**\n",
    "\n",
    "Fill in the blanks and check your answers!\n",
    "\n",
    "1. The formula for a linear regression model is $ f(w, b, x) = \\_ \\cdot x + \\_ $.\n",
    "2. The cost function $ J(w, b) $ measures the **\\_\\_\\_\\_** between predicted and actual values.\n",
    "3. The derivative of $ J(w, b) $ with respect to $ w $ includes the **\\_\\_\\_\\_** term $ x^i $, but the derivative with respect to $ b $ does not.\n",
    "4. A **\\_\\_\\_\\_** cost function ensures gradient descent converges to the global minimum.\n",
    "5. The two key parameters updated in gradient descent are $ \\_\\_\\_\\_$ and $ \\_\\_\\_\\_$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Answer Key**\n",
    "\n",
    "1. $ w $, $ b $\n",
    "2. error/difference\n",
    "3. input feature\n",
    "4. convex\n",
    "5. $ w $, $ b $\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
