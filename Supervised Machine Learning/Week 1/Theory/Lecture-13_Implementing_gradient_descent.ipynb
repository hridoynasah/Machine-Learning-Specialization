{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Gradient Descent**\n",
    "\n",
    "---\n",
    "\n",
    "### **Introduction to Gradient Descent**\n",
    "\n",
    "Gradient Descent is an optimization algorithm used to find the best values for parameters (such as $ w $ and $ b $) in machine learning models. The goal of Gradient Descent is to **minimize the cost function** $ J(w, b) $, which measures how well the model fits the data.\n",
    "\n",
    "Think of it like this: Imagine you are standing on a mountain, and your goal is to reach the lowest point in the valley. You take small steps downhill until you reach a place where going further doesn’t make much difference. This process is similar to how Gradient Descent works—it takes steps in the direction that **reduces the cost function** until it reaches a minimum.\n",
    "\n",
    "---\n",
    "\n",
    "### **Breaking Down the Gradient Descent Equation**\n",
    "\n",
    "The update formula for **one parameter $ w $** in Gradient Descent is:\n",
    "\n",
    "$$\n",
    "w = w - \\alpha \\cdot \\frac{d}{dw} J(w, b)\n",
    "$$\n",
    "\n",
    "Let’s break this down:\n",
    "\n",
    "1. **Current value of $ w $**: We start with some initial guess for $ w $.\n",
    "2. **Step size $ \\alpha $ (Learning Rate)**: This is a small positive number (like 0.01) that determines how big a step we take downhill.\n",
    "3. **Derivative term $ \\frac{d}{dw} J(w, b) $**: This tells us the slope of the cost function at the current value of $ w $, which indicates the direction we should move to reduce the cost.\n",
    "\n",
    "We update $ w $ by subtracting a fraction ($ \\alpha $) of the derivative. This process is repeated until $ w $ stops changing much, meaning we have reached the minimum.\n",
    "\n",
    "Similarly, for the second parameter **$ b $** (bias term), we use:\n",
    "\n",
    "$$\n",
    "b = b - \\alpha \\cdot \\frac{d}{db} J(w, b)\n",
    "$$\n",
    "\n",
    "This means we also adjust $ b $ in the direction that minimizes the cost function.\n",
    "\n",
    "---\n",
    "\n",
    "### **Understanding the Equal Sign (\"=\") in Programming vs. Mathematics**\n",
    "\n",
    "In programming, the equal sign (`=`) is called the **assignment operator**, which means \"store the value on the right into the variable on the left.\"\n",
    "\n",
    "For example:\n",
    "\n",
    "- **`a = c`**: This means \"store the value of `c` into `a`.\"\n",
    "- **`a = a + 1`**: This means \"increase the value of `a` by 1 and store the new value back in `a`.\"\n",
    "\n",
    "However, in mathematics:\n",
    "\n",
    "- The equal sign (`=`) represents a **truth assertion** (a statement that is always true).\n",
    "- Writing **`a = a + 1`** in mathematics doesn’t make sense because it’s always false.\n",
    "\n",
    "To test equality in programming, we use **double equal signs (`==`)** instead:\n",
    "\n",
    "- **`a == c`**: This checks if `a` and `c` have the same value.\n",
    "\n",
    "---\n",
    "\n",
    "### **Learning Rate $ \\alpha $ and Its Importance**\n",
    "\n",
    "$ \\alpha $ is called the **learning rate**, and it controls how big a step we take in Gradient Descent:\n",
    "\n",
    "- **If $ \\alpha $ is too large**: The steps are too big, and we might \"jump over\" the minimum and never find the best solution.\n",
    "- **If $ \\alpha $ is too small**: The steps are too tiny, and it will take a long time to reach the minimum.\n",
    "\n",
    "A good learning rate is typically a small positive number, like **0.01 or 0.001**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Understanding the Derivative Term**\n",
    "\n",
    "The **derivative** (also called the **gradient**) tells us the slope of the cost function at a particular point.\n",
    "\n",
    "- **If the derivative is positive**: The cost function is increasing, so we need to move **left (decrease $ w $)**.\n",
    "- **If the derivative is negative**: The cost function is decreasing, so we need to move **right (increase $ w $)**.\n",
    "\n",
    "Since we **subtract** the gradient in Gradient Descent, this ensures we move **downhill** toward the minimum of the cost function.\n",
    "\n",
    "---\n",
    "\n",
    "### **Gradient Descent for Multiple Parameters**\n",
    "\n",
    "In real models, we often have multiple parameters, like $ w $ and $ b $. We update both parameters **simultaneously** using:\n",
    "\n",
    "$$\n",
    "w = w - \\alpha \\cdot \\frac{d}{dw} J(w, b)\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = b - \\alpha \\cdot \\frac{d}{db} J(w, b)\n",
    "$$\n",
    "\n",
    "We repeat this process until the parameters stop changing significantly, meaning we have reached the optimal values.\n",
    "\n",
    "---\n",
    "\n",
    "### **Simultaneous vs. Non-Simultaneous Updates**\n",
    "\n",
    "#### **Correct Implementation (Simultaneous Update)**\n",
    "\n",
    "The correct way to update $ w $ and $ b $ is:\n",
    "\n",
    "1. **Compute temporary values** before updating:\n",
    "\n",
    "   $$\n",
    "   \\text{temp\\_w} = w - \\alpha \\cdot \\frac{d}{dw} J(w, b)\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\text{temp\\_b} = b - \\alpha \\cdot \\frac{d}{db} J(w, b)\n",
    "   $$\n",
    "\n",
    "2. **Update both variables at the same time**:\n",
    "   $$\n",
    "   w = \\text{temp\\_w}\n",
    "   $$\n",
    "   $$\n",
    "   b = \\text{temp\\_b}\n",
    "   $$\n",
    "\n",
    "## This ensures that both updates use the original values of $ w $ and $ b $, keeping the process mathematically correct.\n",
    "\n",
    "![A](images/L13_1.png)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Incorrect Implementation (Non-Simultaneous Update)**\n",
    "\n",
    "If we update $ w $ before computing the update for $ b $, it changes the cost function before computing $ b $’s update:\n",
    "\n",
    "1. Compute $ \\text{temp_w} $, then update $ w $ immediately.\n",
    "2. Compute $ \\text{temp_b} $, but now **it uses the new $ w $** instead of the original.\n",
    "\n",
    "This leads to incorrect results and makes the optimization process unstable.\n",
    "\n",
    "---\n",
    "\n",
    "### **Gradient Descent Convergence**\n",
    "\n",
    "We continue updating $ w $ and $ b $ **until they stop changing significantly**. This means we have reached a **local minimum**, where the cost function is as small as possible.\n",
    "\n",
    "Convergence happens when:\n",
    "\n",
    "- The gradient (derivative) becomes **close to zero**.\n",
    "- $ w $ and $ b $ no longer change much with each step.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "1. **Gradient Descent** is an optimization algorithm used to find the best parameters by minimizing the cost function.\n",
    "2. **Equation:** $ w = w - \\alpha \\cdot \\frac{d}{dw} J(w, b) $, and similarly for $ b $.\n",
    "3. **Equal Sign Differences:** In programming, `=` is for **assignment**, while in math, it’s a **truth assertion**.\n",
    "4. **Learning Rate $ \\alpha $** controls step size:\n",
    "   - Too big → Overshooting the minimum.\n",
    "   - Too small → Slow learning.\n",
    "5. **Derivative $ \\frac{d}{dw} J(w, b) $** tells us which direction to move:\n",
    "   - Positive → Move left.\n",
    "   - Negative → Move right.\n",
    "6. **Simultaneous Update is Correct:**\n",
    "   - Compute **temp values** first.\n",
    "   - Update both parameters **at the same time**.\n",
    "7. **Gradient Descent repeats** until the parameters stop changing significantly (**convergence**).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
