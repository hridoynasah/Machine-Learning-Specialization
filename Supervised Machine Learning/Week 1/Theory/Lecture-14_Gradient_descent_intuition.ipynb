{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Gradient Descent Intuition**\n",
    "\n",
    "#### **1. What is Gradient Descent?**\n",
    "\n",
    "Gradient descent is a method used in machine learning to adjust parameters (like $ w $ and $ b $) to minimize a cost function. The cost function tells us how well our model is performing. The goal of gradient descent is to find the lowest point of the cost function, meaning the best possible values for $ w $ and $ b $ that make predictions as accurate as possible.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Understanding the Learning Rate $ \\alpha $**\n",
    "\n",
    "- The symbol $ \\alpha $ (alpha) is called the **learning rate**.\n",
    "- The learning rate controls **how big of a step** we take when updating the parameters $ w $ and $ b $.\n",
    "- If $ \\alpha $ is too big, we might **overshoot** the minimum and never settle at the lowest point.\n",
    "- If $ \\alpha $ is too small, the steps will be tiny, making the learning process **very slow**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. What is the Derivative?**\n",
    "\n",
    "- The **derivative** tells us the slope of a function at a particular point.\n",
    "- The notation $ \\frac{d}{dw} $ means **taking the derivative** of the function with respect to $ w $.\n",
    "- The derivative tells us:\n",
    "  - If we should **increase** or **decrease** $ w $.\n",
    "  - How **big of a step** we should take.\n",
    "\n",
    "In more advanced mathematics, this is actually called a **partial derivative**, but for now, weâ€™ll just call it a derivative.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. How Gradient Descent Works Step by Step**\n",
    "\n",
    "Let's simplify the problem and assume we only have **one parameter**, $ w $, and a cost function $ J(w) $.\n",
    "\n",
    "The update rule in gradient descent is:\n",
    "\n",
    "$$\n",
    "w = w - \\alpha \\times \\frac{d}{dw} J(w)\n",
    "$$\n",
    "\n",
    "This means:\n",
    "\n",
    "1. Take the current value of $ w $.\n",
    "2. Subtract a small step ($ \\alpha $) **times** the slope of the cost function at that point.\n",
    "3. Repeat the process until we reach the minimum.\n",
    "\n",
    "---\n",
    "\n",
    "![A](images/L14_1.png)\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Visualizing Gradient Descent with a Graph**\n",
    "\n",
    "##### **Case 1: When We Start from the Right Side**\n",
    "\n",
    "- Imagine a graph where the **horizontal axis** is $ w $ and the **vertical axis** is $ J(w) $ (cost function).\n",
    "- We start from a random point on the graph.\n",
    "- The **slope of the tangent line** at that point is **positive**.\n",
    "- A **positive slope** means:\n",
    "  - $ \\frac{d}{dw} J(w) > 0 $ (the cost is increasing as $ w $ increases).\n",
    "  - Using the update rule: $ w = w - \\alpha \\times \\text{(positive number)} $.\n",
    "  - This **decreases** $ w $, moving left towards the minimum.\n",
    "  - The cost function $ J(w) $ decreases, which is what we want.\n",
    "\n",
    "##### **Case 2: When We Start from the Left Side**\n",
    "\n",
    "- If we start on the **left side**, the slope of the tangent line is **negative**.\n",
    "- A **negative slope** means:\n",
    "  - $ \\frac{d}{dw} J(w) < 0 $ (the cost is decreasing as $ w $ increases).\n",
    "  - Using the update rule: $ w = w - \\alpha \\times \\text{(negative number)} $.\n",
    "  - Subtracting a negative number is the same as **adding** a positive number.\n",
    "  - This **increases** $ w $, moving right towards the minimum.\n",
    "  - Again, $ J(w) $ decreases, showing gradient descent is working correctly.\n",
    "\n",
    "---\n",
    "\n",
    "![A](images/L14_2.png)\n",
    "\n",
    "---\n",
    "\n",
    "**Key observation**:  \n",
    "No matter where we start, gradient descent moves us towards the **minimum** of the cost function.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. Why This Process Works**\n",
    "\n",
    "- The derivative **tells us the direction** we should move.\n",
    "- The learning rate **controls the step size** so we don't move too fast or too slow.\n",
    "- Gradient descent **repeats this process** until it reaches the lowest possible point.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Summary**\n",
    "\n",
    "1. **Gradient Descent** is a method to find the best values of parameters by minimizing a cost function.\n",
    "2. The **learning rate $ \\alpha $** determines how big each step is.\n",
    "3. The **derivative** tells us whether to increase or decrease $ w $.\n",
    "4. If the **slope is positive**, gradient descent **decreases $ w $**.\n",
    "5. If the **slope is negative**, gradient descent **increases $ w $**.\n",
    "6. The process **continues until the cost function reaches its minimum**.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
