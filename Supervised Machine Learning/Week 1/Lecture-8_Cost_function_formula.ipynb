{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression: Cost Function and Its Role\n",
    "\n",
    "In linear regression, one of the first essential steps is defining the **cost function**, which evaluates how well our model is performing. This function helps us adjust the model to improve its predictions.\n",
    "\n",
    "#### The Training Set\n",
    "\n",
    "We start with a **training set** that contains:\n",
    "\n",
    "- Input features $ x $\n",
    "- Output targets $ y $\n",
    "\n",
    "The model we're going to use is a linear function:\n",
    "\n",
    "$$\n",
    "f_{w,b} (x) = wx + b\n",
    "$$\n",
    "\n",
    "Here, $ w $ and $ b $ are called the **parameters** of the model. These parameters are adjusted during training to improve the model's predictions.\n",
    "\n",
    "- $ w $ is referred to as the **weight** or **coefficient**, and it determines the slope of the line.\n",
    "- $ b $ is the **bias** or **intercept**, and it determines where the line crosses the y-axis.\n",
    "\n",
    "---\n",
    "\n",
    "![Example](images/TrainingSet.png)\n",
    "\n",
    "---\n",
    "\n",
    "#### Effect of Parameters $ w $ and $ b $\n",
    "\n",
    "The values of $ w $ and $ b $ affect the model's output in the following ways:\n",
    "\n",
    "- When $ w = 0 $ and $ b = 1.5 $, the function $ f(x) $ is constant:\n",
    "\n",
    "  $$\n",
    "  f(x) = 0 \\cdot x + 1.5 = 1.5\n",
    "  $$\n",
    "\n",
    "  This creates a **horizontal line** at $ y = 1.5 $.\n",
    "\n",
    "- When $ w = 0.5 $ and $ b = 0 $, the function is:\n",
    "\n",
    "  $$\n",
    "  f(x) = 0.5x\n",
    "  $$\n",
    "\n",
    "  The line passes through the origin (0, 0), and the slope is $ 0.5 $.\n",
    "\n",
    "- When $ w = 0.5 $ and $ b = 1 $, the function is:\n",
    "\n",
    "  $$\n",
    "  f(x) = 0.5x + 1\n",
    "  $$\n",
    "\n",
    "  The line has a slope of $ 0.5 $ and crosses the y-axis at $ y = 1 $.\n",
    "\n",
    "---\n",
    "\n",
    "![My Image](images/LinearGraph.png)\n",
    "\n",
    "---\n",
    "\n",
    "#### Training Set Example\n",
    "\n",
    "Let’s assume a dataset with **m** examples, each consisting of an input $ x^i $ and target $ y^i $. For a given input $ x^i $, the model predicts $ \\hat{y}^i $ using the linear function $ f\\_{w,b}(x^i) = wx^i + b $.\n",
    "\n",
    "#### The Goal of Linear Regression\n",
    "\n",
    "The objective is to find values of $ w $ and $ b $ such that the predicted values $ \\hat{y}^i $ are as close as possible to the true target values $ y^i $ for all the training examples.\n",
    "\n",
    "#### Cost Function: Measuring the Error\n",
    "\n",
    "The **cost function** measures how far off the predictions $ \\hat{y}^i $ are from the true values $ y^i $. The error for each prediction is:\n",
    "\n",
    "$$\n",
    "\\text{Error} = \\hat{y}^i - y^i\n",
    "$$\n",
    "\n",
    "To quantify this error, we square the difference, and for each example $ i $, the squared error is:\n",
    "\n",
    "$$\n",
    "\\text{Squared Error} = (\\hat{y}^i - y^i)^2\n",
    "$$\n",
    "\n",
    "#### Total and Average Error\n",
    "\n",
    "To measure the overall error across all training examples, we sum the squared errors:\n",
    "\n",
    "$$\n",
    "\\text{Total Squared Error} = \\sum_{i=1}^m (\\hat{y}^i - y^i)^2\n",
    "$$\n",
    "\n",
    "Since the size of the training set (denoted as $ m $) influences the total error, we normalize this by dividing by $ m $ to get the **average squared error**:\n",
    "\n",
    "$$\n",
    "\\text{Average Squared Error} = \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}^i - y^i)^2\n",
    "$$\n",
    "\n",
    "To make the formula cleaner, we add a division by 2:\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{2m} \\sum_{i=1}^m (\\hat{y}^i - y^i)^2\n",
    "$$\n",
    "\n",
    "This is the **cost function** $ J(w, b) $, also called the **squared error cost function**. The reason it's called squared error is that we're squaring the difference between the predicted and actual values.\n",
    "\n",
    "#### Why the Division by 2?\n",
    "\n",
    "The division by 2 doesn't change the behavior of the cost function but simplifies later calculations, particularly when performing optimization to minimize $ J(w, b) $.\n",
    "\n",
    "#### Interpretation of the Cost Function\n",
    "\n",
    "The cost function $ J(w, b) $ measures how well the model's predictions match the true targets. The goal in linear regression is to adjust the parameters $ w $ and $ b $ so that the cost function $ J(w, b) $ is as small as possible.\n",
    "\n",
    "- If $ J(w, b) $ is **large**, it means the model's predictions are far from the actual values.\n",
    "- If $ J(w, b) $ is **small**, the model’s predictions are close to the actual values.\n",
    "\n",
    "---\n",
    "\n",
    "![Example](images/CFF.png)\n",
    "\n",
    "---\n",
    "\n",
    "In the next steps, we will use optimization techniques, such as **gradient descent**, to adjust $ w $ and $ b $ to minimize $ J(w, b) $.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
