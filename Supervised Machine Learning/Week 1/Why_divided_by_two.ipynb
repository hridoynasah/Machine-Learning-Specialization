{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The division of the **cost function** by 2 is a common practice in machine learning, particularly in linear regression, when using the **mean squared error (MSE)** or **sum of squared errors (SSE)** as the cost function. The reason for dividing by 2 is purely for **mathematical convenience** during optimization, specifically when taking derivatives. Here's the explanation::\n",
    "\n",
    "---\n",
    "\n",
    "### Cost Function (Mean Squared Error):\n",
    "\n",
    "The cost function for linear regression is often written as:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $m$ = number of training examples\n",
    "- $h_\\theta(x^{(i)})$ = predicted value for the $i$-th example\n",
    "- $y^{(i)}$ = actual value for the $i$-th example\n",
    "- $\\theta$ = parameters of the model\n",
    "\n",
    "---\n",
    "\n",
    "### Why Divide by 2?\n",
    "\n",
    "When you take the derivative of the cost function $J(\\theta)$ with respect to the parameters $\\theta$, the squared term $(h_\\theta(x^{(i)}) - y^{(i)})^2$ will produce a factor of 2 in the derivative:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta} \\left( \\frac{1}{2} (h_\\theta(x^{(i)}) - y^{(i)})^2 \\right) = (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot \\frac{\\partial h_\\theta(x^{(i)})}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "The $\\frac{1}{2}$ cancels out the factor of 2 that arises from the derivative of the squared term, simplifying the gradient calculation. This makes the math cleaner and avoids unnecessary constants in the gradient descent update rule.\n",
    "\n",
    "---\n",
    "\n",
    "### Gradient Descent Update Rule:\n",
    "\n",
    "The gradient descent update rule for linear regression is:\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}\n",
    "$$\n",
    "\n",
    "Without the $\\frac{1}{2}$, the derivative would include an extra factor of 2, which would need to be accounted for in the learning rate $\\alpha$. By including $\\frac{1}{2}$ in the cost function, the gradient descent update rule becomes simpler and more intuitive.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "\n",
    "The division by 2 is **not necessary** for the correctness of the algorithm but is included for **mathematical convenience** during differentiation. It simplifies the gradient calculation and makes the gradient descent update rule cleaner.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
