{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Gradient Descent**\n",
    "\n",
    "In this lecture, we are introduced to an important concept in machine learning called **gradient descent**. This is an optimization algorithm that helps us find the best values for parameters like $ w $ (weight) and $ b $ (bias) so that our cost function $ J(w, b) $ is minimized.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Understanding the Need for Gradient Descent**\n",
    "\n",
    "In previous discussions, we saw how the cost function $ J(w, b) $ changes based on different values of $ w $ and $ b $. The goal is to find the best values of these parameters that result in the **smallest possible cost**.\n",
    "\n",
    "- Simply trying random values for $ w $ and $ b $ and checking their cost is not an efficient way to find the minimum.\n",
    "- We need a **systematic approach** to reach the lowest possible value of $ J(w, b) $.\n",
    "- This is where **gradient descent** comes in.\n",
    "\n",
    "---\n",
    "\n",
    "![Example](images/L12_1.png)\n",
    "\n",
    "---\n",
    "\n",
    "### **2. What is Gradient Descent?**\n",
    "\n",
    "Gradient descent is an **optimization algorithm** that helps find the minimum of a function. It is widely used in machine learning, not just for **linear regression**, but also for **training deep learning models**.\n",
    "\n",
    "#### **2.1 How Gradient Descent Works**\n",
    "\n",
    "1. **Start with some initial values** for $ w $ and $ b $.\n",
    "\n",
    "   - A common choice is to start with both $ w $ and $ b $ as **zero**.\n",
    "   - The initial values don’t matter much for linear regression, but they do matter in other complex models.\n",
    "\n",
    "2. **Iteratively update $ w $ and $ b $ to reduce the cost function**.\n",
    "\n",
    "   - In each step, gradient descent makes small adjustments to $ w $ and $ b $ so that $ J(w, b) $ **decreases**.\n",
    "   - The goal is to keep moving towards the **lowest** possible value of $ J(w, b) $.\n",
    "\n",
    "3. **Keep repeating this process until the cost function is minimized**.\n",
    "   - At some point, the changes become very small, meaning we have reached the lowest possible value (or something very close to it).\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Gradient Descent as a General Algorithm**\n",
    "\n",
    "Gradient descent is not just for linear regression. It can be used for **any function** that we need to minimize.\n",
    "\n",
    "- For example, in deep learning, we deal with cost functions that depend on **many parameters**.\n",
    "- If we have a function $ J(w_1, w_2, ..., w_n, b) $, gradient descent can find the best values for **all these parameters**.\n",
    "\n",
    "Thus, gradient descent is a **general-purpose optimization algorithm** that applies to many machine learning models.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Visualizing Gradient Descent**\n",
    "\n",
    "To better understand gradient descent, let’s imagine a **hilly outdoor park or a golf course** where:\n",
    "\n",
    "- The **hills** are high values of $ J(w, b) $.\n",
    "- The **valleys** are low values of $ J(w, b) $, which we want to reach.\n",
    "\n",
    "Imagine **standing on a hill**. Your goal is to **walk downhill** to the lowest possible point (the valley).\n",
    "\n",
    "#### **4.1 Steps in the Process**\n",
    "\n",
    "1. **Look around in all directions (360° spin).**\n",
    "\n",
    "   - Ask yourself: **\"Which direction is the steepest way down?\"**\n",
    "   - This direction is called the **direction of steepest descent**.\n",
    "\n",
    "2. **Take a small step in that direction.**\n",
    "\n",
    "   - This small step is like adjusting $ w $ and $ b $ slightly to move towards a lower cost.\n",
    "\n",
    "3. **Repeat the process.**\n",
    "\n",
    "   - At your new position, look around again and find the steepest way down.\n",
    "   - Take another small step in that direction.\n",
    "\n",
    "4. **Keep going until you reach the lowest point.**\n",
    "   - Once you are in the **valley**, you can stop because you have found a local minimum.\n",
    "\n",
    "---\n",
    "\n",
    "![Example](images/L12_2.png)\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Local Minima and Why They Matter**\n",
    "\n",
    "Sometimes, the landscape of the cost function is **complex**. Instead of one deep valley, there might be **multiple valleys** at different locations.\n",
    "\n",
    "- **Local Minimum:** A valley where the algorithm gets stuck and cannot go lower.\n",
    "- **Global Minimum:** The absolute lowest valley among all.\n",
    "\n",
    "#### **5.1 What Happens with Different Starting Points?**\n",
    "\n",
    "- If you start on the **left side of a hill**, you might end up in one valley.\n",
    "- If you start on the **right side**, you might end up in a different valley.\n",
    "\n",
    "**Key Insight:** The **starting point matters** in complex models like neural networks. If we start at different points, we might reach different minima.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Key Takeaways from the Lecture**\n",
    "\n",
    "- **Gradient descent helps us find the best values for parameters (w, b) by minimizing the cost function $ J(w, b) $.**\n",
    "- **We start with initial values (often 0) and update them iteratively.**\n",
    "- **It works for many machine learning problems, not just linear regression.**\n",
    "- **We can visualize it as \"walking down a hill\" step by step to reach a valley (minimum cost).**\n",
    "- **In complex problems, there may be multiple valleys (local minima), so the starting point affects the result.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "Gradient descent is a powerful algorithm that helps optimize machine learning models by minimizing the cost function. It works by making small changes to the parameters $ w $ and $ b $, always moving in the direction that reduces the cost the most. By repeating this process, we eventually reach a minimum point where the cost function is as low as possible. In more complex models, there may be multiple minima, so the starting point of the algorithm can affect where we end up. Understanding gradient descent is **essential** because it is a fundamental tool in machine learning and deep learning.\n",
    "\n",
    "- **Gradient descent** is an optimization algorithm used to minimize the cost function **J(w, b)** by adjusting the parameters **w** and **b** step by step.\n",
    "- It works by computing the **gradient** (steepest downhill direction) and taking small steps toward the lowest cost.\n",
    "- It is widely used not only in **linear regression** but also in **deep learning** and many other machine learning models.\n",
    "- The cost function might have multiple valleys (**local minima**), which means gradient descent might not always find the absolute best minimum.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
