{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function Visualization in Linear Regression\n",
    "\n",
    "## Understanding the Cost Function $J(w, b)$\n",
    "\n",
    "In linear regression, our goal is to find the best-fitting line for a given set of data points. This line is represented by the function:\n",
    "$\n",
    "    f(x) = wx + b\n",
    "$\n",
    "where $w$ is the slope and $b$ is the intercept.\n",
    "\n",
    "To measure how well a particular choice of $w$ and $b$ fits the data, we use a cost function $J(w, b)$. The cost function calculates the difference between the predicted values and the actual values in the dataset, summing up the squared errors:\n",
    "$\n",
    "    J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (f(x_i) - y_i)^2\n",
    "$\n",
    "where:\n",
    "\n",
    "- $m$ is the number of training examples,\n",
    "- $f(x_i)$ is the predicted value for the $i^{th}$ data point,\n",
    "- $y_i$ is the actual target value.\n",
    "\n",
    "The goal is to minimize $J(w, b)$ by selecting the best values for $w$ and $b$.\n",
    "\n",
    "---\n",
    "\n",
    "## Visualizing $w$, $b$, and the Cost Function\n",
    "\n",
    "### Example 1: Poor Fit with High Cost\n",
    "\n",
    "Consider a particular point on the cost function graph where:\n",
    "\n",
    "- $w \\approx -0.15$\n",
    "- $b \\approx 800$\n",
    "\n",
    "This pair of values corresponds to a function:\n",
    "$\n",
    "    f(x) = -0.15x + 800\n",
    "$\n",
    "\n",
    "This function intersects the vertical axis at $b = 800$, and its negative slope indicates a downward trend. However, when compared to the data points in the training set, this line is a poor fit. Many predicted values are far from the actual values. As a result, the cost $J(w, b)$ for this function is relatively high and is positioned far from the minimum on the cost graph.\n",
    "\n",
    "---\n",
    "\n",
    "### Example 2: Slightly Better Fit\n",
    "\n",
    "Now, let’s look at another case where:\n",
    "\n",
    "- $w = 0$\n",
    "- $b \\approx 360$\n",
    "\n",
    "The function in this case is:\n",
    "$\n",
    "    f(x) = 0 \\cdot x + 360 = 360\n",
    "$\n",
    "\n",
    "This results in a flat horizontal line at $y = 360$. While this is still not a great fit, it performs slightly better than the previous example. The corresponding cost is lower but still not at the optimal minimum.\n",
    "\n",
    "---\n",
    "\n",
    "### Example 3: Worse Fit than Before\n",
    "\n",
    "Next, consider a different set of values for $w$ and $b$ that results in a function $f(x)$ with an even worse fit than the previous example. This means that the cost function value is positioned further away from the minimum.\n",
    "\n",
    "---\n",
    "\n",
    "### Example 4: Close to Optimal Fit\n",
    "\n",
    "Finally, let’s examine a case where the function appears to be a good fit:\n",
    "\n",
    "- The line closely follows the trend of the data points.\n",
    "- The sum of squared errors is minimized.\n",
    "- The cost function value is near the center of the smallest contour ellipse on the cost function graph.\n",
    "\n",
    "This function $f(x)$ provides a near-optimal fit, meaning that the sum of squared errors is close to the lowest possible value.\n",
    "\n",
    "---\n",
    "\n",
    "## Interactive Cost Function Visualization\n",
    "\n",
    "In practice, we can visualize and interact with the cost function:\n",
    "\n",
    "- A **contour plot** shows how different values of $w$ and $b$ affect the cost function.\n",
    "- A **3D surface plot** provides an interactive way to understand how cost changes with parameter values.\n",
    "- By clicking different points on the contour plot, we can observe how the corresponding line $f(x)$ behaves in relation to the training data.\n",
    "- The interactive console allows users to experiment with different parameter values and see their impact on cost.\n",
    "\n",
    "---\n",
    "\n",
    "## Gradient Descent: Finding the Best Fit Automatically\n",
    "\n",
    "Rather than manually adjusting $w$ and $b$ by trial and error, we use an algorithm called **gradient descent**. Gradient descent is an optimization algorithm that systematically updates the parameters in a way that minimizes the cost function $J(w, b)$. This algorithm is fundamental in training not only linear regression models but also complex machine learning models, including deep learning networks.\n",
    "\n",
    "### How Gradient Descent Works:\n",
    "\n",
    "1. Start with initial values for $w$ and $b$.\n",
    "2. Compute the gradient (partial derivatives) of the cost function:\n",
    "   $\n",
    "       \\frac{\\partial J}{\\partial w}, \\quad \\frac{\\partial J}{\\partial b}\n",
    "   $\n",
    "3. Update $w$ and $b$ iteratively using the learning rate $\\alpha$:\n",
    "   $\n",
    "       w := w - \\alpha \\frac{\\partial J}{\\partial w}\n",
    "   $\n",
    "   $\n",
    "       b := b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "   $\n",
    "4. Repeat the process until $J(w, b)$ converges to a minimum value.\n",
    "\n",
    "Gradient descent efficiently finds the optimal values of $w$ and $b$, leading to the best-fitting line.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- The cost function $J(w, b)$ measures how well a linear regression model fits the data by summing squared errors.\n",
    "- Different values of $w$ and $b$ result in different cost values, which can be visualized using contour and surface plots.\n",
    "- Poor choices of $w$ and $b$ lead to high costs, while better choices bring the cost closer to the minimum.\n",
    "- Instead of manually adjusting parameters, gradient descent is used to efficiently find the optimal values.\n",
    "- Gradient descent updates $w$ and $b$ iteratively to minimize $J(w, b)$, making it a fundamental optimization algorithm in machine learning.\n",
    "\n",
    "By understanding cost function visualization and gradient descent, we gain insight into how linear regression models are trained and optimized for accurate predictions.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
