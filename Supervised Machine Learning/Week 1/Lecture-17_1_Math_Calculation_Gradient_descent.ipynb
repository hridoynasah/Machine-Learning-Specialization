{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Mathematical calculation of gradient descent for linear regression using the specified cost function.**\n",
    "\n",
    "### 1. Linear Regression Model Equation\n",
    "\n",
    "The linear regression model is given by:\n",
    "$$ \\hat{y}^{(i)} = w \\cdot x^{(i)} + b $$\n",
    "where:\n",
    "\n",
    "- $ \\hat{y}^{(i)} $ is the predicted output for the $ i $-th training example.\n",
    "- $ x^{(i)} $ is the input feature for the $ i $-th training example.\n",
    "- $ w $ is the weight (or slope).\n",
    "- $ b $ is the bias (or intercept).\n",
    "\n",
    "### 2. Cost Function\n",
    "\n",
    "The cost function (mean squared error) measures how well the model fits the data:\n",
    "$$ J(w, b) = \\frac{1}{2m} \\sum\\_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2 $$\n",
    "where:\n",
    "\n",
    "- $ m $ is the number of training examples.\n",
    "- $ y^{(i)} $ is the actual output for the $ i $-th training example.\n",
    "- $ \\hat{y}^{(i)} = w \\cdot x^{(i)} + b $ is the predicted output for the $ i $-th training example.\n",
    "\n",
    "### 3. Calculation of Gradient Descent Equation\n",
    "\n",
    "#### Step 1: Compute the Partial Derivatives\n",
    "\n",
    "We need to compute the partial derivatives of the cost function with respect to $ w $ and $ b $.\n",
    "\n",
    "**Partial derivative with respect to $ w $:**\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{\\partial}{\\partial w} \\left[ \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2 \\right] $$\n",
    "\n",
    "Using the chain rule:\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m} \\sum*{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)}) \\cdot \\frac{\\partial \\hat{y}^{(i)}}{\\partial w} $$\n",
    "Since $ \\hat{y}^{(i)} = w \\cdot x^{(i)} + b $, we have:\n",
    "$$ \\frac{\\partial \\hat{y}^{(i)}}{\\partial w} = x^{(i)} $$\n",
    "Thus:\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m} \\sum*{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)}) \\cdot x^{(i)} $$\n",
    "\n",
    "**Partial derivative with respect to $ b $:**\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{\\partial}{\\partial b} \\left[ \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2 \\right] $$\n",
    "\n",
    "Using the chain rule:\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum*{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)}) \\cdot \\frac{\\partial \\hat{y}^{(i)}}{\\partial b} $$\n",
    "Since $ \\hat{y}^{(i)} = w \\cdot x^{(i)} + b $, we have:\n",
    "$$ \\frac{\\partial \\hat{y}^{(i)}}{\\partial b} = 1 $$\n",
    "Thus:\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum*{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)}) $$\n",
    "\n",
    "#### Step 2: Update the Parameters\n",
    "\n",
    "The parameters $ w $ and $ b $ are updated using the following equations:\n",
    "$$ w := w - \\alpha \\cdot \\frac{\\partial J}{\\partial w} $$\n",
    "$$ b := b - \\alpha \\cdot \\frac{\\partial J}{\\partial b} $$\n",
    "where $ \\alpha $ is the learning rate, a hyperparameter that determines the step size at each iteration.\n",
    "\n",
    "Substituting the partial derivatives:\n",
    "$$ w := w - \\alpha \\cdot \\frac{1}{m} \\sum*{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)}) \\cdot x^{(i)} $$\n",
    "$$ b := b - \\alpha \\cdot \\frac{1}{m} \\sum*{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)}) $$\n",
    "\n",
    "### Summary\n",
    "\n",
    "To summarize, the gradient descent algorithm for linear regression involves:\n",
    "\n",
    "1. Computing the partial derivatives of the cost function with respect to the parameters $ w $ and $ b $.\n",
    "2. Updating the parameters $ w $ and $ b $ iteratively using the computed gradients and a chosen learning rate $ \\alpha $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Break down of the calculation**\n",
    "\n",
    "#### Expression:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w} \\left[ \\frac{1}{2m} \\sum_{i=1}^{m} (w \\cdot x^{(i)} + b - y^{(i)})^2 \\right] $$\n",
    "\n",
    "#### Step-by-Step Breakdown:\n",
    "\n",
    "1. **Outer Function:**\n",
    "\n",
    "   - The outer function is a sum of squared terms divided by $ 2m $.\n",
    "   - We need to differentiate this with respect to $ w $.\n",
    "\n",
    "2. **Inner Function:**\n",
    "\n",
    "   - The inner function is $ (w \\cdot x^{(i)} + b - y^{(i)})^2 $.\n",
    "   - When differentiating this with respect to $ w $, we use the chain rule.\n",
    "\n",
    "3. **Chain Rule Application:**\n",
    "\n",
    "   - Differentiate the square term:\n",
    "     $$\\frac{\\partial}{\\partial w} (w \\cdot x^{(i)} + b - y^{(i)})^2 = 2(w \\cdot x^{(i)} + b - y^{(i)}) \\cdot \\frac{\\partial}{\\partial w} (w \\cdot x^{(i)} + b - y^{(i)}) $$\n",
    "   - The derivative of $ w \\cdot x^{(i)} + b - y^{(i)} $ with respect to $ w $ is $ x^{(i)} $.\n",
    "\n",
    "4. **Combining Results:**\n",
    "   - Putting it all together:\n",
    "     $$\\frac{\\partial}{\\partial w} \\left[ \\frac{1}{2m} \\sum_{i=1}^{m} (w \\cdot x^{(i)} + b - y^{(i)})^2 \\right] = \\frac{1}{2m} \\sum_{i=1}^{m} 2(w \\cdot x^{(i)} + b - y^{(i)}) \\cdot x^{(i)} $$\n",
    "   - Simplifying:\n",
    "     $$= \\frac{1}{m} \\sum_{i=1}^{m} (w \\cdot x^{(i)} + b - y^{(i)}) \\cdot x^{(i)} $$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
