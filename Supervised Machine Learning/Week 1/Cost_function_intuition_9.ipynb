{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function Explanation\n",
    "\n",
    "In this lecture, we’re exploring the cost function in detail and how it helps find the best parameters for a model, especially in linear regression. Let's break it down step by step.\n",
    "\n",
    "#### Recap: The Linear Model\n",
    "\n",
    "You want to fit a straight line to your training data. Your model is defined as:\n",
    "\n",
    "$$\n",
    "f_{w, b}(x) = wx + b\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ w $ and $ b $ are the model parameters (weights and bias, respectively),\n",
    "- $ x $ is the input feature, and\n",
    "- $ f\\_{w, b}(x) $ is the predicted output.\n",
    "\n",
    "For different values of $ w $ and $ b $, you get different straight lines, and you want to find the values of $ w $ and $ b $ that minimize the difference between the predicted values $ f\\_{w, b}(x) $ and the actual values $ y $.\n",
    "\n",
    "#### What the Cost Function Does\n",
    "\n",
    "The cost function $ J(w, b) $ measures the difference between the model's predictions and the true values. The goal is to minimize this cost function. We want to find $ w $ and $ b $ such that the cost is as small as possible:\n",
    "\n",
    "$$\n",
    "\\min_{w, b} J(w, b)\n",
    "$$\n",
    "\n",
    "In this video, we'll simplify the model and look at how the cost function works when we only have one parameter $ w $ and set $ b = 0 $.\n",
    "\n",
    "#### Simplified Model: $ b = 0 $\n",
    "\n",
    "For simplicity, let's assume that the bias term $ b $ is set to zero, so the model becomes:\n",
    "\n",
    "$$\n",
    "f_w(x) = wx\n",
    "$$\n",
    "\n",
    "This leaves us with just one parameter $ w $. The cost function now depends only on $ w $, and is defined as the sum of squared errors between the predicted values and the true values:\n",
    "\n",
    "$$\n",
    "J(w) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left(f_w(x^i) - y^i\\right)^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ m $ is the number of training examples,\n",
    "- $ f_w(x^i) $ is the predicted value for the $ i $-th training example,\n",
    "- $ y^i $ is the true value for the $ i $-th training example.\n",
    "\n",
    "Our goal is to find the value of $ w $ that minimizes this cost function $ J(w) $.\n",
    "\n",
    "#### Visualizing the Cost Function\n",
    "\n",
    "Let’s now visualize how the cost function changes as we choose different values for $ w $. We’ll plot both the function $ f_w(x) $ and the cost function $ J(w) $.\n",
    "\n",
    "1. **For $ w = 1 $**:\n",
    "\n",
    "   - The model $ f_w(x) = x $ fits the data perfectly for the points (1, 1), (2, 2), and (3, 3), as the line passes exactly through all of them.\n",
    "   - The cost function $ J(1) = 0 $, as there is no error between the predicted values and the actual values.\n",
    "\n",
    "2. **For $ w = 0.5 $**:\n",
    "\n",
    "   - The model $ f_w(x) = 0.5x $ now underestimates the values for each point.\n",
    "   - The squared errors for each point are:\n",
    "     - For $ x = 1 $, the predicted value is $ 0.5 $, so the error is $ (0.5 - 1)^2 = 0.25 $.\n",
    "     - For $ x = 2 $, the predicted value is $ 1 $, so the error is $ (1 - 2)^2 = 1 $.\n",
    "     - For $ x = 3 $, the predicted value is $ 1.5 $, so the error is $ (1.5 - 3)^2 = 2.25 $.\n",
    "   - The total cost $ J(0.5) $ is the sum of these errors:\n",
    "\n",
    "   $$\n",
    "   J(0.5) = \\frac{1}{2 \\times 3} \\left(0.25 + 1 + 2.25\\right) = 0.58\n",
    "   $$\n",
    "\n",
    "3. **For $ w = 0 $**:\n",
    "\n",
    "   - The model $ f_w(x) = 0 $ is just a horizontal line at $ y = 0 $, and the errors are quite large.\n",
    "   - The squared errors for each point are:\n",
    "     - For $ x = 1 $, the error is $ (0 - 1)^2 = 1 $,\n",
    "     - For $ x = 2 $, the error is $ (0 - 2)^2 = 4 $,\n",
    "     - For $ x = 3 $, the error is $ (0 - 3)^2 = 9 $.\n",
    "   - The total cost $ J(0) $ is:\n",
    "\n",
    "   $$\n",
    "   J(0) = \\frac{1}{2 \\times 3} \\left(1 + 4 + 9\\right) = 2.33\n",
    "   $$\n",
    "\n",
    "#### How the Cost Function Relates to the Line\n",
    "\n",
    "As we change $ w $, the line $ f_w(x) $ changes its slope, and the cost function $ J(w) $ reflects how well the model fits the data. The closer the predicted values are to the true values, the smaller the cost $ J(w) $.\n",
    "\n",
    "- When $ w = 1 $, the model fits the data perfectly, and the cost is zero.\n",
    "- As $ w $ moves away from 1, the cost increases because the model's predictions get further from the true values.\n",
    "\n",
    "#### Minimizing the Cost Function\n",
    "\n",
    "To find the optimal value for $ w $, we need to minimize the cost function $ J(w) $. This can be done using optimization techniques like gradient descent. The value of $ w $ that minimizes the cost function will give us the best fit for the data.\n",
    "\n",
    "In this example, the value of $ w = 1 $ results in the smallest cost and a perfect fit to the data.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- The cost function $ J(w) $ measures the difference between the model's predictions and the actual values.\n",
    "- We use the cost function to find the best parameters (in this case, $ w $) that minimize the error.\n",
    "- When the model fits the data well, the cost is small.\n",
    "- The goal of linear regression is to minimize $ J(w) $ to find the best fit line.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
