{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **📚 Understanding Gradient Descent for Multiple Linear Regression with Vectorization**\n",
    "\n",
    "<div style=\"text-align:center;\">     <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\"> </div>\n",
    "\n",
    "#### 🚀 **Introduction**\n",
    "\n",
    "In this lesson, we’ll learn how **Gradient Descent** works for **Multiple Linear Regression** and how we can implement it efficiently using **Vectorization**. We’ll also explore an alternative method called the **Normal Equation**.\n",
    "\n",
    "<div style=\"text-align:center;\">     <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\"> </div>\n",
    "\n",
    "#### 🧠 **What is Multiple Linear Regression?**\n",
    "\n",
    "##### 🔹 **Basic Idea**\n",
    "\n",
    "Multiple Linear Regression is a way to predict an **output (y)** using multiple **input features (x₁, x₂, ..., xₙ)**. Instead of just one feature like in Simple Linear Regression, we now have **n features**.\n",
    "\n",
    "##### 🔹 **Equation Representation**\n",
    "\n",
    "If we have **n features**, the equation for **Multiple Linear Regression** is:\n",
    "\n",
    "$$\n",
    " f_{w, b}(x) = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ w_1, w_2, ..., w_n $ are the weights (parameters) of the model.\n",
    "- $ b $ is the bias term.\n",
    "- $ x_1, x_2, ..., x_n $ are the feature values.\n",
    "\n",
    "<div style=\"text-align:center;\">     <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\"> </div>\n",
    "\n",
    "#### 📌 **Vectorized Representation**\n",
    "\n",
    "Instead of treating each **w** separately, we can **combine them into a vector**:\n",
    "\n",
    "$$\n",
    " w = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So now, we can write our equation as:\n",
    "\n",
    "$$\n",
    " f_{w, b}(x) = w^T x + b\n",
    "$$\n",
    "\n",
    "where **w and x are vectors** and **w^T x means the dot product** of the two.\n",
    "\n",
    "This makes calculations much **faster** and **more efficient** with NumPy! 🚀\n",
    "\n",
    "<div style=\"text-align:center;\">     <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\"> </div>\n",
    "\n",
    "#### 🎯 **Cost Function**\n",
    "\n",
    "To measure how well our model is performing, we use the **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\n",
    " J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( f_{w, b}(x^{(i)}) - y^{(i)} \\right)^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ m $ = number of training examples.\n",
    "- $ f\\_{w, b}(x^{(i)}) $ = predicted value.\n",
    "- $ y^{(i)} $ = actual value.\n",
    "\n",
    "The goal is to **minimize J(w, b)** using **Gradient Descent**.\n",
    "\n",
    "<div style=\"text-align:center;\">     <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\"> </div>\n",
    "\n",
    "#### 🔥 **Gradient Descent for Multiple Linear Regression**\n",
    "\n",
    "##### 🔹 **Basic Idea**\n",
    "\n",
    "Gradient Descent is an **optimization algorithm** that updates weights to minimize the cost function.\n",
    "\n",
    "##### 🔹 **Update Rule**\n",
    "\n",
    "We update the parameters **w and b** using the following formula:\n",
    "\n",
    "$$\n",
    " w_j := w_j - \\alpha \\frac{\\partial J(w, b)}{\\partial w_j}\n",
    "$$\n",
    "\n",
    "$$\n",
    " b := b - \\alpha \\frac{\\partial J(w, b)}{\\partial b}\n",
    "$$\n",
    "\n",
    "where **α (alpha)** is the **learning rate** (controls step size).\n",
    "\n",
    "##### 🔹 **Gradient Calculation**\n",
    "\n",
    "For **each feature** $ w_j $:\n",
    "\n",
    "$$\n",
    " \\frac{\\partial J(w, b)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( f_{w, b}(x^{(i)}) - y^{(i)} \\right) x_j^{(i)}\n",
    "$$\n",
    "\n",
    "For **b**:\n",
    "\n",
    "$$\n",
    " \\frac{\\partial J(w, b)}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( f_{w, b}(x^{(i)}) - y^{(i)} \\right)\n",
    "$$\n",
    "\n",
    "This means we update **all weights** and **b** simultaneously.\n",
    "\n",
    "<div style=\"text-align:center;\">     <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\"> </div>\n",
    "\n",
    "#### ⚡ **Vectorized Implementation**\n",
    "\n",
    "Instead of using loops, we use **matrix operations**:\n",
    "\n",
    "$$\n",
    " w := w - \\alpha \\frac{1}{m} X^T (Xw + b - Y)\n",
    "$$\n",
    "\n",
    "$$\n",
    " b := b - \\alpha \\frac{1}{m} \\sum (Xw + b - Y)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ X $ = feature matrix.\n",
    "- $ Y $ = target values.\n",
    "- $ X^T $ = transpose of $ X $.\n",
    "\n",
    "This **reduces computation time** significantly! ⚡\n",
    "\n",
    "<div style=\"text-align:center;\">     <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\"> </div>\n",
    "\n",
    "#### 🏆 **Alternative Method: The Normal Equation**\n",
    "\n",
    "Instead of using **Gradient Descent**, we can compute $ w $ **directly** using:\n",
    "\n",
    "$$\n",
    " w = (X^T X)^{-1} X^T Y\n",
    "$$\n",
    "\n",
    "##### 📉 **Why is it not used often?**\n",
    "\n",
    "1. ❌ **Doesn’t work for other models** (Only works for Linear Regression)\n",
    "2. ❌ **Computationally expensive** for large datasets\n",
    "3. ✅ **But... it doesn’t need iterations**!\n",
    "\n",
    "<div style=\"text-align:center;\">     <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\"> </div>\n",
    "\n",
    "#### 🎯 **Summary**\n",
    "\n",
    "✅ **Multiple Linear Regression** predicts an output using multiple input features.\n",
    "✅ We use **vectorization** to make computations faster.\n",
    "✅ The **cost function** (MSE) measures how well our model is performing.\n",
    "✅ **Gradient Descent** updates weights iteratively to minimize the cost.\n",
    "✅ **Vectorized Gradient Descent** makes training efficient.\n",
    "✅ **The Normal Equation** finds weights in one step but is slow for large datasets.\n",
    "\n",
    "<div style=\"text-align:center;\">     <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\"> </div>\n",
    "\n",
    "#### 🎯 **Interactive Quiz (MCQs)**\n",
    "\n",
    "##### **1️⃣ What is the purpose of the cost function in Linear Regression?**\n",
    "\n",
    "A) To increase the weights 🔼  \n",
    "B) To decrease the number of features 📉  \n",
    "C) To measure how well the model is performing ✅  \n",
    "D) To stop the training process ❌\n",
    "\n",
    "##### **2️⃣ What does the learning rate (α) do in Gradient Descent?**\n",
    "\n",
    "A) Controls the step size when updating weights ✅  \n",
    "B) Controls the number of iterations ❌  \n",
    "C) Decreases the cost function manually ❌  \n",
    "D) Adjusts the number of features ❌\n",
    "\n",
    "##### **3️⃣ Which of the following is TRUE about the Normal Equation?**\n",
    "\n",
    "A) It requires multiple iterations ❌  \n",
    "B) It is efficient for large datasets ❌  \n",
    "C) It can be used for other models like Logistic Regression ❌  \n",
    "D) It directly computes the optimal weights ✅\n",
    "\n",
    "##### **4️⃣ Why is vectorization important in Machine Learning?**\n",
    "\n",
    "A) It makes computations slower ❌  \n",
    "B) It improves computational efficiency ✅  \n",
    "C) It reduces the number of training examples ❌  \n",
    "D) It makes models less accurate ❌\n",
    "\n",
    "<div style=\"text-align:center;\">     <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\"> </div>\n",
    "\n",
    "#### ✅ **Quiz Answers**\n",
    "\n",
    "1️⃣ C) To measure how well the model is performing  \n",
    "2️⃣ A) Controls the step size when updating weights  \n",
    "3️⃣ D) It directly computes the optimal weights  \n",
    "4️⃣ B) It improves computational efficiency\n",
    "\n",
    "<div style=\"text-align:center;\">     <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\"> </div>\n",
    "\n",
    "### 🎉 That’s it! Now you understand **Gradient Descent for Multiple Linear Regression** and how to make it efficient with **vectorization**! 🚀\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
