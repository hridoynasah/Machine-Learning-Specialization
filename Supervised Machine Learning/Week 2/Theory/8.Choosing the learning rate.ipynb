{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ **Understanding Learning Rate in Gradient Descent**  \n",
    "\n",
    "Choosing the **right learning rate (Î±)** is **crucial** for training machine learning models efficiently. If it's **too small**, the training will be **slow**. If it's **too large**, the model **might not converge** and could behave unpredictably. Let's dive deep into how to choose an optimal learning rate!  \n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\">\n",
    "</div>  \n",
    "\n",
    "### ğŸ¯ **What Happens When the Learning Rate is Too High?**  \n",
    "\n",
    "When **Î± is too large**, the updates to parameters **overshoot** the minimum point. Instead of gradually decreasing, it keeps jumping back and forth, making training unstable.  \n",
    "\n",
    "ğŸ”´ **Example**:  \n",
    "- Suppose we are minimizing a **cost function $ J $**.  \n",
    "- If the **learning rate is too large**, the **gradient descent update moves too far**, missing the optimal value.  \n",
    "- This results in a **zig-zag** movement that never settles, or even worse, the cost function **increases** instead of decreasing.  \n",
    "\n",
    "âœ **Solution**: **Use a smaller learning rate** to prevent wild oscillations.  \n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\">\n",
    "</div>  \n",
    "\n",
    "### ğŸ¢ **What Happens When the Learning Rate is Too Small?**  \n",
    "\n",
    "When **Î± is too small**, the model takes **tiny steps** towards the minimum, which means:  \n",
    "\n",
    "- It will take **too many iterations** to converge.  \n",
    "- The **training process becomes very slow**.  \n",
    "- Computational power is wasted.  \n",
    "\n",
    "âœ **Solution**: **Increase the learning rate** a bit so that training converges faster while still being stable.  \n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\">\n",
    "</div>  \n",
    "\n",
    "### ğŸ” **Debugging Gradient Descent**  \n",
    "\n",
    "If your cost function $ J $ is not decreasing **even with a small Î±**, then:  \n",
    "\n",
    "1ï¸âƒ£ **There may be a bug in the code** âŒ  \n",
    "2ï¸âƒ£ **Check the update formula** for gradient descent:  \n",
    "   $$\n",
    "   w_1 = w_1 - \\alpha \\times \\text{(derivative)}\n",
    "   $$\n",
    "   âš  If you mistakenly use **$ + $ instead of $ - $**, the model will **diverge instead of converging**!  \n",
    "\n",
    "âœ **Fix**: Always use the **minus sign ( - )** in gradient descent.  \n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\">\n",
    "</div>  \n",
    "\n",
    "### ğŸ”¥ **Finding the Best Learning Rate**  \n",
    "\n",
    "The best way to find a good learning rate is by **experimenting with different values**:  \n",
    "\n",
    "âœ… Start with a **small value** like **0.001**.  \n",
    "âœ… Try increasing it **3 times** at each step (**0.003, 0.01, 0.03, etc.**).  \n",
    "âœ… Plot the cost function $ J $ over time and see how it behaves.  \n",
    "âœ… Pick the **largest stable** learning rate for faster convergence.  \n",
    "\n",
    "ğŸ’¡ **Tip**: The learning rate should be **big enough** to make progress but **not so big** that the model overshoots.  \n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\">\n",
    "</div>  \n",
    "\n",
    "### âœ¨ **Summary**  \n",
    "\n",
    "âœ” **Too large $ Î± $** â†’ Overshooting, no convergence ğŸš€  \n",
    "âœ” **Too small $ Î± $** â†’ Slow learning, waste of time ğŸ¢  \n",
    "âœ” **Bug in code** â†’ Gradient descent may increase cost instead of decreasing ğŸ“‰  \n",
    "âœ” **Best learning rate** â†’ Try different values (0.001 â†’ 0.003 â†’ 0.01 â†’ 0.03...) and **find the largest stable one** ğŸ¯  \n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\">\n",
    "</div>  \n",
    "\n",
    "### ğŸ¯ **Interactive Notes (MCQ)**  \n",
    "\n",
    "1ï¸âƒ£ **What happens if the learning rate is too large?**  \n",
    "   a) Model learns quickly and converges faster  \n",
    "   b) Model overshoots the minimum and oscillates  \n",
    "   c) Model stops learning  \n",
    "\n",
    "2ï¸âƒ£ **What should you do if gradient descent is not decreasing the cost function?**  \n",
    "   a) Increase the learning rate significantly  \n",
    "   b) Check if you used **$ - \\alpha \\times \\text{derivative} $** correctly  \n",
    "   c) Stop training  \n",
    "\n",
    "3ï¸âƒ£ **What is the correct update rule for gradient descent?**  \n",
    "   a) $ w_1 = w_1 + \\alpha \\times \\text{(derivative)} $  \n",
    "   b) $ w_1 = w_1 - \\alpha \\times \\text{(derivative)} $  \n",
    "   c) $ w_1 = w_1 - \\frac{\\alpha}{2} \\times \\text{(derivative)} $  \n",
    "\n",
    "4ï¸âƒ£ **If the learning rate is too small, what happens?**  \n",
    "   a) Model learns slowly and takes too many iterations  \n",
    "   b) Model immediately finds the global minimum  \n",
    "   c) The cost function increases  \n",
    "\n",
    "5ï¸âƒ£ **How should you experiment with learning rates?**  \n",
    "   a) Try random values between 0.001 and 1  \n",
    "   b) Try values increasing by **3 times** at each step (e.g., 0.001 â†’ 0.003 â†’ 0.01 â†’ 0.03...)  \n",
    "   c) Keep using the smallest learning rate possible  \n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **Answers**  \n",
    "1ï¸âƒ£ **b**  \n",
    "2ï¸âƒ£ **b**  \n",
    "3ï¸âƒ£ **b**  \n",
    "4ï¸âƒ£ **a**  \n",
    "5ï¸âƒ£ **b**  \n",
    "\n",
    "Hope this explanation makes learning rate **super clear** for you! ğŸš€ğŸ’¡"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
