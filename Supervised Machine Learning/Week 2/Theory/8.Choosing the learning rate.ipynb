{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚀 **Understanding Learning Rate in Gradient Descent**  \n",
    "\n",
    "Choosing the **right learning rate (α)** is **crucial** for training machine learning models efficiently. If it's **too small**, the training will be **slow**. If it's **too large**, the model **might not converge** and could behave unpredictably. Let's dive deep into how to choose an optimal learning rate!  \n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\">\n",
    "</div>  \n",
    "\n",
    "### 🎯 **What Happens When the Learning Rate is Too High?**  \n",
    "\n",
    "When **α is too large**, the updates to parameters **overshoot** the minimum point. Instead of gradually decreasing, it keeps jumping back and forth, making training unstable.  \n",
    "\n",
    "🔴 **Example**:  \n",
    "- Suppose we are minimizing a **cost function $ J $**.  \n",
    "- If the **learning rate is too large**, the **gradient descent update moves too far**, missing the optimal value.  \n",
    "- This results in a **zig-zag** movement that never settles, or even worse, the cost function **increases** instead of decreasing.  \n",
    "\n",
    "✏ **Solution**: **Use a smaller learning rate** to prevent wild oscillations.  \n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\">\n",
    "</div>  \n",
    "\n",
    "### 🐢 **What Happens When the Learning Rate is Too Small?**  \n",
    "\n",
    "When **α is too small**, the model takes **tiny steps** towards the minimum, which means:  \n",
    "\n",
    "- It will take **too many iterations** to converge.  \n",
    "- The **training process becomes very slow**.  \n",
    "- Computational power is wasted.  \n",
    "\n",
    "✏ **Solution**: **Increase the learning rate** a bit so that training converges faster while still being stable.  \n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\">\n",
    "</div>  \n",
    "\n",
    "### 🔍 **Debugging Gradient Descent**  \n",
    "\n",
    "If your cost function $ J $ is not decreasing **even with a small α**, then:  \n",
    "\n",
    "1️⃣ **There may be a bug in the code** ❌  \n",
    "2️⃣ **Check the update formula** for gradient descent:  \n",
    "   $$\n",
    "   w_1 = w_1 - \\alpha \\times \\text{(derivative)}\n",
    "   $$\n",
    "   ⚠ If you mistakenly use **$ + $ instead of $ - $**, the model will **diverge instead of converging**!  \n",
    "\n",
    "✏ **Fix**: Always use the **minus sign ( - )** in gradient descent.  \n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\">\n",
    "</div>  \n",
    "\n",
    "### 🔥 **Finding the Best Learning Rate**  \n",
    "\n",
    "The best way to find a good learning rate is by **experimenting with different values**:  \n",
    "\n",
    "✅ Start with a **small value** like **0.001**.  \n",
    "✅ Try increasing it **3 times** at each step (**0.003, 0.01, 0.03, etc.**).  \n",
    "✅ Plot the cost function $ J $ over time and see how it behaves.  \n",
    "✅ Pick the **largest stable** learning rate for faster convergence.  \n",
    "\n",
    "💡 **Tip**: The learning rate should be **big enough** to make progress but **not so big** that the model overshoots.  \n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\">\n",
    "</div>  \n",
    "\n",
    "### ✨ **Summary**  \n",
    "\n",
    "✔ **Too large $ α $** → Overshooting, no convergence 🚀  \n",
    "✔ **Too small $ α $** → Slow learning, waste of time 🐢  \n",
    "✔ **Bug in code** → Gradient descent may increase cost instead of decreasing 📉  \n",
    "✔ **Best learning rate** → Try different values (0.001 → 0.003 → 0.01 → 0.03...) and **find the largest stable one** 🎯  \n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\">\n",
    "</div>  \n",
    "\n",
    "### 🎯 **Interactive Notes (MCQ)**  \n",
    "\n",
    "1️⃣ **What happens if the learning rate is too large?**  \n",
    "   a) Model learns quickly and converges faster  \n",
    "   b) Model overshoots the minimum and oscillates  \n",
    "   c) Model stops learning  \n",
    "\n",
    "2️⃣ **What should you do if gradient descent is not decreasing the cost function?**  \n",
    "   a) Increase the learning rate significantly  \n",
    "   b) Check if you used **$ - \\alpha \\times \\text{derivative} $** correctly  \n",
    "   c) Stop training  \n",
    "\n",
    "3️⃣ **What is the correct update rule for gradient descent?**  \n",
    "   a) $ w_1 = w_1 + \\alpha \\times \\text{(derivative)} $  \n",
    "   b) $ w_1 = w_1 - \\alpha \\times \\text{(derivative)} $  \n",
    "   c) $ w_1 = w_1 - \\frac{\\alpha}{2} \\times \\text{(derivative)} $  \n",
    "\n",
    "4️⃣ **If the learning rate is too small, what happens?**  \n",
    "   a) Model learns slowly and takes too many iterations  \n",
    "   b) Model immediately finds the global minimum  \n",
    "   c) The cost function increases  \n",
    "\n",
    "5️⃣ **How should you experiment with learning rates?**  \n",
    "   a) Try random values between 0.001 and 1  \n",
    "   b) Try values increasing by **3 times** at each step (e.g., 0.001 → 0.003 → 0.01 → 0.03...)  \n",
    "   c) Keep using the smallest learning rate possible  \n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Answers**  \n",
    "1️⃣ **b**  \n",
    "2️⃣ **b**  \n",
    "3️⃣ **b**  \n",
    "4️⃣ **a**  \n",
    "5️⃣ **b**  \n",
    "\n",
    "Hope this explanation makes learning rate **super clear** for you! 🚀💡"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
