{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Œ **Understanding Multiple Linear Regression:**\n",
    "\n",
    "#### ğŸŒŸ **Step 1: What is Linear Regression?**\n",
    "\n",
    "In the last week, we learned about **linear regression**, which is used to predict a value using one variable. For example, we predicted the price of a house using **only** the size of the house.  \n",
    "The equation was:\n",
    "\n",
    "$$\n",
    "f_{w,b}(x) = w \\cdot x + b\n",
    "$$\n",
    "\n",
    "Here:  \n",
    "âœ… **$ x $** = size of the house (input variable)  \n",
    "âœ… **$ w $** = weight (how much the size affects the price)  \n",
    "âœ… **$ b $** = bias (starting price of a house)\n",
    "\n",
    "This was **simple linear regression** because we had **only one feature**.\n",
    "\n",
    "<div style=\"text-align:center;\">     <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\"> </div>\n",
    "\n",
    "#### ğŸ”¥ **Step 2: What if We Have More variables?**\n",
    "\n",
    "Let's say we want to predict house prices more accurately. We now have **4 variables**:\n",
    "\n",
    "1ï¸âƒ£ **$ X_1 $** = Size of the house (sq ft)  \n",
    "2ï¸âƒ£ **$ X_2 $** = Number of bedrooms  \n",
    "3ï¸âƒ£ **$ X_3 $** = Number of floors  \n",
    "4ï¸âƒ£ **$ X_4 $** = Age of the house (years old)\n",
    "\n",
    "With more variables, our equation changes to:\n",
    "\n",
    "$$\n",
    "f_{w,b}(x) = w_1 X_1 + w_2 X_2 + w_3 X_3 + w_4 X_4 + b\n",
    "$$\n",
    "\n",
    "Now, instead of just size, we use **more details** about the house to make better predictions! ğŸ¯\n",
    "\n",
    "For example, if we use:  \n",
    "âœ… $ X_1 = 1416 $ (size)  \n",
    "âœ… $ X_2 = 3 $ (bedrooms)  \n",
    "âœ… $ X_3 = 2 $ (floors)  \n",
    "âœ… $ X_4 = 40 $ (age)\n",
    "\n",
    "Then the price will be calculated as:\n",
    "\n",
    "$$\n",
    "\\hat{y} = 0.1(1416) + 4(3) + 10(2) - 2(40) + 80\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 141.6 + 12 + 20 - 80 + 80\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 173.6 \\text{ (in thousands of dollars, meaning \\$173,600)}\n",
    "$$\n",
    "\n",
    "ğŸš€ This is called **multiple linear regression** because we use multiple variables to predict the price!\n",
    "\n",
    "<div style=\"text-align:center;\">     <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\"> </div>\n",
    "\n",
    "#### ğŸ¤” **Step 3: Understanding Notation**\n",
    "\n",
    "We introduce **new notations** to keep things clean:\n",
    "\n",
    "âœ… **$ X_j $** â†’ $ j^{th} $ feature [A general way to refer to any feature (e.g., $ X_1, X_2 $ etc.)]  \n",
    "âœ… **$ n $** â†’ Number of variables  \n",
    "âœ… **$ \\vec{X}^{(i)} $** â†’ variables of $ i^{th} $ training example (e.g., the second houseâ€™s variables) **_[This is represented by row vector]_**  \n",
    "âœ… **$ X^{(i)}\\_j $** â†’ The $ j^{th} $ feature of the $ i^{th} $ house\n",
    "\n",
    "Example:  \n",
    "ğŸ‘‰ $ X^{(2)}\\_3 = 2 $ â†’ This means that for the **2nd house**, the **3rd feature** (number of floors) is **2**.\n",
    "\n",
    "<div style=\"text-align:center;\">     <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\"> </div>\n",
    "\n",
    "#### ğŸ¯ **Step 4: Using Vectors to Simplify Equations**\n",
    "\n",
    "Instead of writing out long equations, we use **vectors** (a fancy word for a list of numbers):\n",
    "\n",
    "##### ğŸ”¹ **Weight Vector ($ W $)**\n",
    "\n",
    "$$\n",
    "\\vec{W} = [w_1, w_2, w_3, ..., w_n]\n",
    "$$\n",
    "\n",
    "ğŸ‘‰ A list of all weights for each feature\n",
    "\n",
    "##### ğŸ”¹ **Feature Vector ($ X $)**\n",
    "\n",
    "$$\n",
    "\\vec{X} = [X_1, X_2, X_3, ..., X_n]\n",
    "$$\n",
    "\n",
    "ğŸ‘‰ A list of all feature values\n",
    "\n",
    "Now, the equation becomes:\n",
    "\n",
    "$$\n",
    "f_{\\vec{w},b}(\\vec{x}) = \\vec{W} \\cdot \\vec{X} + b\n",
    "$$\n",
    "\n",
    "ğŸ“Œ The **dot (Â·)** means **dot product**, which is just multiplying corresponding values and summing them up!\n",
    "\n",
    "$$\n",
    "\\vec{W} \\cdot \\vec{X} = w_1X_1 + w_2X_2 + w_3X_3 + ... + w_nX_n\n",
    "$$\n",
    "\n",
    "ğŸ’¡ **This notation makes calculations faster and cleaner!**\n",
    "\n",
    "<div style=\"text-align:center;\">     <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\"> </div>\n",
    "\n",
    "#### âœ¨ **Summary**\n",
    "\n",
    "âœ… **Simple Linear Regression** â†’ Uses **one feature** to predict output.  \n",
    "âœ… **Multiple Linear Regression** â†’ Uses **multiple variables** for better accuracy.  \n",
    "âœ… **Vector Notation** â†’ Simplifies equations and makes computations faster.  \n",
    "âœ… **Dot Product** â†’ A mathematical trick to multiply and sum values efficiently.\n",
    "\n",
    "ğŸš€ In the next lesson, weâ€™ll learn about **vectorization**, a technique to make computations even **faster**!\n",
    "\n",
    "<div style=\"text-align:center;\">     <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\"> </div>\n",
    "\n",
    "### ğŸ“– **Interactive Notes (MCQ)**\n",
    "\n",
    "##### **Q1: What is the difference between simple and multiple linear regression?**\n",
    "\n",
    "ğŸ”˜ A. Simple uses multiple variables, multiple uses only one  \n",
    "ğŸ”˜ B. Simple uses one feature, multiple uses multiple variables  \n",
    "ğŸ”˜ C. There is no difference  \n",
    "ğŸ”˜ D. Multiple linear regression is only for deep learning\n",
    "\n",
    "##### **Q2: What does the dot product ($ \\vec{W} \\cdot \\vec{X} $) represent?**\n",
    "\n",
    "ğŸ”˜ A. Adding all variables  \n",
    "ğŸ”˜ B. Multiplying corresponding values and summing them  \n",
    "ğŸ”˜ C. Only multiplying variables  \n",
    "ğŸ”˜ D. Ignoring bias in the model\n",
    "\n",
    "##### **Q3: In the equation $ f\\_{\\vec{w},b}(\\vec{x}) = \\vec{W} \\cdot \\vec{X} + b $, what does $ b $ represent?**\n",
    "\n",
    "ğŸ”˜ A. The number of variables  \n",
    "ğŸ”˜ B. The base value (bias)  \n",
    "ğŸ”˜ C. A feature of the house  \n",
    "ğŸ”˜ D. The weight of the largest feature\n",
    "\n",
    "##### **Q4: What is a vector?**\n",
    "\n",
    "ğŸ”˜ A. A number  \n",
    "ğŸ”˜ B. A list of numbers  \n",
    "ğŸ”˜ C. A type of function  \n",
    "ğŸ”˜ D. A machine learning model\n",
    "\n",
    "##### **Q5: Which feature might reduce the house price?**\n",
    "\n",
    "ğŸ”˜ A. More square feet  \n",
    "ğŸ”˜ B. More bedrooms  \n",
    "ğŸ”˜ C. More floors  \n",
    "ğŸ”˜ D. More age\n",
    "\n",
    "<div style=\"text-align:center;\">     <img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" alt=\"green-divider\"> </div>\n",
    "\n",
    "ğŸ’¡ **Answers to Check:**  \n",
    "âœ… Q1 â†’ **B**  \n",
    "âœ… Q2 â†’ **B**  \n",
    "âœ… Q3 â†’ **B**  \n",
    "âœ… Q4 â†’ **B**  \n",
    "âœ… Q5 â†’ **D**\n",
    "\n",
    "ğŸ‰ Thatâ€™s it! You now have a **clear** understanding of multiple linear regression! ğŸš€ Letâ€™s move on to **vectorization** next!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
