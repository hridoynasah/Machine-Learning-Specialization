{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Œ **Understanding Gradient Descent Convergence**\n",
    "\n",
    "#### ğŸš€ **Introduction**\n",
    "\n",
    "Gradient Descent is an algorithm that helps us find the best values for parameters like $ w $ and $ b $ to minimize a cost function $ J $. But how do we know if it's actually working? ğŸ¤”\n",
    "\n",
    "A simple way to check is by **plotting the cost function $ J $ at each iteration** and observing how it changes. This plot is called a **learning curve**, and it helps us understand if gradient descent is doing a good job or not.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“‰ **How Do We Know Gradient Descent is Converging?**\n",
    "\n",
    "When we run gradient descent, we update our parameters $ w $ and $ b $ over and over again. After each update, we calculate the cost function $ J(w, b) $ and plot it.\n",
    "\n",
    "#### **ğŸ”¹ What does a good learning curve look like?**\n",
    "\n",
    "- The **x-axis** â Number of iterations (how many times we updated $ w $ and $ b $)\n",
    "- The **y-axis** â Value of the cost function $ J $\n",
    "\n",
    "A well-running gradient descent should show a **decreasing cost function** like this: ğŸ“‰  \n",
    "![learning curve](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)\n",
    "\n",
    "This means:\n",
    "âœ… **Each iteration makes the cost function smaller**  \n",
    "âœ… **The curve eventually flattens out**, meaning we are close to the minimum cost\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ›‘ **What If the Cost Function Increases?**\n",
    "\n",
    "If the cost function $ J $ suddenly **increases** at some point, thatâ€™s a bad sign! ğŸš¨ This could mean:\n",
    "1ï¸âƒ£ **The learning rate $ \\alpha $ is too large** â The updates are too aggressive, jumping past the minimum.  \n",
    "2ï¸âƒ£ **Thereâ€™s a bug in the code** â Check the calculations and formulas again.\n",
    "\n",
    "ğŸ’¡ **Fix:** Try **reducing** the learning rate $ \\alpha $ and observe the learning curve again.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”„ **When Should We Stop Gradient Descent?**\n",
    "\n",
    "If you look at the learning curve, at some point it becomes **almost flat**. This means gradient descent has **converged** (i.e., itâ€™s not learning much anymore).\n",
    "\n",
    "You can:\n",
    "âœ… **Stop training manually** when the curve flattens  \n",
    "âœ… **Use an automatic convergence test** with a small number $ \\epsilon $ (e.g., $ 0.001 $)\n",
    "\n",
    "**ğŸ“Œ Rule of Thumb:**\n",
    "If the cost function decreases by **less than $ \\epsilon $ in one iteration**, it means gradient descent has reached its best point.\n",
    "\n",
    "ğŸ’¡ **Fun Fact:** Some problems might need **30 iterations**, while others might need **100,000 iterations**! ğŸ˜² Thatâ€™s why plotting the learning curve is important.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ **Summary**\n",
    "\n",
    "- âœ… **Gradient descent** is used to minimize the cost function $ J(w, b) $.\n",
    "- âœ… The **learning curve** plots the cost function against the number of iterations.\n",
    "- âœ… If gradient descent is **working well**, the cost function should **always decrease**.\n",
    "- ğŸš¨ If the cost function **increases**, the learning rate might be **too high** or there is a bug.\n",
    "- ğŸ›‘ We can **stop training** when the curve flattens or when the cost function change is **less than $ \\epsilon $**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ **Interactive Notes (MCQs)**\n",
    "\n",
    "Try answering these questions to check your understanding! âœ…\n",
    "\n",
    "### **1ï¸âƒ£ What does a learning curve show?**\n",
    "\n",
    "ğŸ”˜ (A) The change in cost function over iterations  \n",
    "ğŸ”˜ (B) The change in $ w $ and $ b $ values  \n",
    "ğŸ”˜ (C) The accuracy of the model  \n",
    "ğŸ”˜ (D) The training time\n",
    "\n",
    "### **2ï¸âƒ£ What does it mean if the cost function $ J $ suddenly increases?**\n",
    "\n",
    "ğŸ”˜ (A) The learning rate $ \\alpha $ is too small  \n",
    "ğŸ”˜ (B) The model is learning properly  \n",
    "ğŸ”˜ (C) The learning rate $ \\alpha $ is too large or there is a bug  \n",
    "ğŸ”˜ (D) The model has converged\n",
    "\n",
    "### **3ï¸âƒ£ What is the purpose of setting $ \\epsilon $?**\n",
    "\n",
    "ğŸ”˜ (A) To determine the best learning rate  \n",
    "ğŸ”˜ (B) To automatically detect when gradient descent has converged  \n",
    "ğŸ”˜ (C) To speed up gradient descent  \n",
    "ğŸ”˜ (D) To make the learning curve look smoother\n",
    "\n",
    "### **4ï¸âƒ£ How do we know if gradient descent has converged?**\n",
    "\n",
    "ğŸ”˜ (A) The cost function keeps decreasing sharply  \n",
    "ğŸ”˜ (B) The cost function stops decreasing significantly  \n",
    "ğŸ”˜ (C) The values of $ w $ and $ b $ increase  \n",
    "ğŸ”˜ (D) The number of iterations is greater than 1000\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… **Answers**\n",
    "\n",
    "1ï¸âƒ£ **(A)** - The learning curve shows the change in the cost function over iterations.  \n",
    "2ï¸âƒ£ **(C)** - If $ J $ increases, the learning rate $ \\alpha $ is too large, or there is a bug.  \n",
    "3ï¸âƒ£ **(B)** - $ \\epsilon $ helps decide when to stop training automatically.  \n",
    "4ï¸âƒ£ **(B)** - When the cost function stops decreasing significantly, gradient descent has converged.\n",
    "\n",
    "---\n",
    "\n",
    "Hope this makes it easy to understand gradient descent convergence! ğŸš€ğŸ“ˆ Let me know if you have any questions! ğŸ˜Š\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
