{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📌 **Understanding Gradient Descent Convergence**\n",
    "\n",
    "#### 🚀 **Introduction**\n",
    "\n",
    "Gradient Descent is an algorithm that helps us find the best values for parameters like $ w $ and $ b $ to minimize a cost function $ J $. But how do we know if it's actually working? 🤔\n",
    "\n",
    "A simple way to check is by **plotting the cost function $ J $ at each iteration** and observing how it changes. This plot is called a **learning curve**, and it helps us understand if gradient descent is doing a good job or not.\n",
    "\n",
    "---\n",
    "\n",
    "### 📉 **How Do We Know Gradient Descent is Converging?**\n",
    "\n",
    "When we run gradient descent, we update our parameters $ w $ and $ b $ over and over again. After each update, we calculate the cost function $ J(w, b) $ and plot it.\n",
    "\n",
    "#### **🔹 What does a good learning curve look like?**\n",
    "\n",
    "- The **x-axis** ➝ Number of iterations (how many times we updated $ w $ and $ b $)\n",
    "- The **y-axis** ➝ Value of the cost function $ J $\n",
    "\n",
    "A well-running gradient descent should show a **decreasing cost function** like this: 📉  \n",
    "![learning curve](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)\n",
    "\n",
    "This means:\n",
    "✅ **Each iteration makes the cost function smaller**  \n",
    "✅ **The curve eventually flattens out**, meaning we are close to the minimum cost\n",
    "\n",
    "---\n",
    "\n",
    "### 🛑 **What If the Cost Function Increases?**\n",
    "\n",
    "If the cost function $ J $ suddenly **increases** at some point, that’s a bad sign! 🚨 This could mean:\n",
    "1️⃣ **The learning rate $ \\alpha $ is too large** ➝ The updates are too aggressive, jumping past the minimum.  \n",
    "2️⃣ **There’s a bug in the code** ➝ Check the calculations and formulas again.\n",
    "\n",
    "💡 **Fix:** Try **reducing** the learning rate $ \\alpha $ and observe the learning curve again.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 **When Should We Stop Gradient Descent?**\n",
    "\n",
    "If you look at the learning curve, at some point it becomes **almost flat**. This means gradient descent has **converged** (i.e., it’s not learning much anymore).\n",
    "\n",
    "You can:\n",
    "✅ **Stop training manually** when the curve flattens  \n",
    "✅ **Use an automatic convergence test** with a small number $ \\epsilon $ (e.g., $ 0.001 $)\n",
    "\n",
    "**📌 Rule of Thumb:**\n",
    "If the cost function decreases by **less than $ \\epsilon $ in one iteration**, it means gradient descent has reached its best point.\n",
    "\n",
    "💡 **Fun Fact:** Some problems might need **30 iterations**, while others might need **100,000 iterations**! 😲 That’s why plotting the learning curve is important.\n",
    "\n",
    "---\n",
    "\n",
    "### 📝 **Summary**\n",
    "\n",
    "- ✅ **Gradient descent** is used to minimize the cost function $ J(w, b) $.\n",
    "- ✅ The **learning curve** plots the cost function against the number of iterations.\n",
    "- ✅ If gradient descent is **working well**, the cost function should **always decrease**.\n",
    "- 🚨 If the cost function **increases**, the learning rate might be **too high** or there is a bug.\n",
    "- 🛑 We can **stop training** when the curve flattens or when the cost function change is **less than $ \\epsilon $**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Interactive Notes (MCQs)**\n",
    "\n",
    "Try answering these questions to check your understanding! ✅\n",
    "\n",
    "### **1️⃣ What does a learning curve show?**\n",
    "\n",
    "🔘 (A) The change in cost function over iterations  \n",
    "🔘 (B) The change in $ w $ and $ b $ values  \n",
    "🔘 (C) The accuracy of the model  \n",
    "🔘 (D) The training time\n",
    "\n",
    "### **2️⃣ What does it mean if the cost function $ J $ suddenly increases?**\n",
    "\n",
    "🔘 (A) The learning rate $ \\alpha $ is too small  \n",
    "🔘 (B) The model is learning properly  \n",
    "🔘 (C) The learning rate $ \\alpha $ is too large or there is a bug  \n",
    "🔘 (D) The model has converged\n",
    "\n",
    "### **3️⃣ What is the purpose of setting $ \\epsilon $?**\n",
    "\n",
    "🔘 (A) To determine the best learning rate  \n",
    "🔘 (B) To automatically detect when gradient descent has converged  \n",
    "🔘 (C) To speed up gradient descent  \n",
    "🔘 (D) To make the learning curve look smoother\n",
    "\n",
    "### **4️⃣ How do we know if gradient descent has converged?**\n",
    "\n",
    "🔘 (A) The cost function keeps decreasing sharply  \n",
    "🔘 (B) The cost function stops decreasing significantly  \n",
    "🔘 (C) The values of $ w $ and $ b $ increase  \n",
    "🔘 (D) The number of iterations is greater than 1000\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **Answers**\n",
    "\n",
    "1️⃣ **(A)** - The learning curve shows the change in the cost function over iterations.  \n",
    "2️⃣ **(C)** - If $ J $ increases, the learning rate $ \\alpha $ is too large, or there is a bug.  \n",
    "3️⃣ **(B)** - $ \\epsilon $ helps decide when to stop training automatically.  \n",
    "4️⃣ **(B)** - When the cost function stops decreasing significantly, gradient descent has converged.\n",
    "\n",
    "---\n",
    "\n",
    "Hope this makes it easy to understand gradient descent convergence! 🚀📈 Let me know if you have any questions! 😊\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
